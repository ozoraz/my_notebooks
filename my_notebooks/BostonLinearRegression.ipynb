{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 課題 ボストン住宅価格 線形回帰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 線形回帰とは何か\n",
    "以下の観点をすべて含めて記述しましょう。\n",
    "- 線形回帰とは何か。\n",
    "- 具体的に言うと？\n",
    "- 分類と何か違うのか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 答え：\n",
    "線形回帰とは、  \n",
    "Yが連続値の場合に、データにY = AX + Bというモデルを当てはめる回帰分析のひとつ。YがXの係数Aに対して線形を描く。機械学習においては、Yがラベル、Xが特徴量に当たる。    \n",
    "具体的には、特徴量XをモデルAX + Bに入力すればラベルYを推測できる、というもの。  \n",
    "分類は、Yが連続値ではない場合(離散)を扱うため、Yが連続値である場合を扱う回帰とはこの点で異なる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 必要なライブラリをimportする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データを取得する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 取得データをDataFrameにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(boston.data,columns=boston.feature_names)\n",
    "y = pd.DataFrame(boston.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 説明変数を’LSTAT’のみにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X['LSTAT'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  単回帰と重回帰についての違いを記述せよ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 答え：\n",
    "上述した線形回帰モデルY = AX + Bにおいて、X が1次元ならば単回帰、X が2次元以上ならば重回帰と言う。機械学習においては、特徴量Xが１つだけなら単回帰、２つ以上なら重回帰である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テストデータに分割する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test=train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_1d = LinearRegression()\n",
    "lin_1d.fit(X_train[:,None], Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 決定係数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一次式における'LSTAT'の住宅価格への決定係数は0.43\n"
     ]
    }
   ],
   "source": [
    "score_1d = lin_1d.score(X_test[:,None], Y_test)\n",
    "print(\"一次式における'LSTAT'の住宅価格への決定係数は%.2f\"%(score_1d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 決定係数とは何か記述せよ\n",
    "以下の観点をすべて含めて記述しましょう。\n",
    "- 決定係数とは何か\n",
    "- もっとも説明変数が、目的変数を説明できる場合、決定係数は何になるか\n",
    "- どのように求めることができるか"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 答え：\n",
    "決定係数とは、  \n",
    "学習したモデルについて、説明変数が目的変数のどれくらいを説明できるかを表す値。未知のデータに対する予測精度を測る指標のひとつ。  \n",
    "***『もっとも説明変数が、目的変数を説明できる場合』***、つまり予測値と教師情報に誤差が生じない場合、決定係数は***1***になる。  \n",
    "(予測値−教師情報)の２乗和を(予測値−教師情報の平均)の２乗和で割ったものを、１から減算して、平方根を取って求めることができる(*この他にも、決定係数の定義は複数ある)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 決定係数をいかなる場合も信じても良いか記述せよ(決定係数が高ければ、汎用性があるモデルと言えるか)\n",
    "- 決定係数が正しく評価できない例を答えよ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 答え：\n",
    "決定係数が高ければ、汎用性があるモデルとは、必ずしも言えない。何故ならば、決定係数はあくまで***予め用意できた教師情報に対するモデルの当てはまり***しか示せないからである。つまり、用意できた教師情報以外へのモデルの当てはまりは保証できないので、教師情報以外の巨大な母集団に対してはモデルが当てはまらない可能性がある。  \n",
    "例えば、選挙の出口調査で１００人から投票先を聞いた場合を考える。このうち８０人のデータをトレーニングデータに学習してモデルを作り、２０人のデータをテストデータとして決定係数をとる。ここで、決定係数が高いからといってこのモデルが全体の選挙結果に対して汎用性がある保証はない。何故ならば、モデルの当てはまりが良いのは、あくまで***出口調査した１００人についてのみ***であるからだ。全体の選挙結果について汎用性を求めるなら、この方法では***日本の有権者人口約１億人全てについて***出口調査する必要がある"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2,3,4次式の回帰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "二次式における'LSTAT'の住宅価格への決定係数は0.52\n",
      "三次式における'LSTAT'の住宅価格への決定係数は0.54\n",
      "四次式における'LSTAT'の住宅価格への決定係数は0.57\n"
     ]
    }
   ],
   "source": [
    "lin_2d = LinearRegression()\n",
    "lin_3d = LinearRegression()\n",
    "lin_4d = LinearRegression()\n",
    "\n",
    "degree_2 = PolynomialFeatures(degree=2)\n",
    "degree_3 = PolynomialFeatures(degree=3)\n",
    "degree_4 = PolynomialFeatures(degree=4)\n",
    "\n",
    "x_train_2 = degree_2.fit_transform(X_train[:,None])\n",
    "x_train_3 = degree_3.fit_transform(X_train[:,None])\n",
    "x_train_4 = degree_4.fit_transform(X_train[:,None])\n",
    "\n",
    "lin_2d.fit(x_train_2, Y_train)\n",
    "lin_3d.fit(x_train_3, Y_train)\n",
    "lin_4d.fit(x_train_4, Y_train)\n",
    "\n",
    "x_test_2 = degree_2.fit_transform(X_test[:,None])\n",
    "x_test_3 = degree_3.fit_transform(X_test[:,None])\n",
    "x_test_4 = degree_4.fit_transform(X_test[:,None])\n",
    "\n",
    "score_2d = lin_2d.score(x_test_2, Y_test)\n",
    "score_3d = lin_3d.score(x_test_3, Y_test)\n",
    "score_4d = lin_4d.score(x_test_4, Y_test)\n",
    "\n",
    "print(\"二次式における'LSTAT'の住宅価格への決定係数は%.2f\"%(score_2d))\n",
    "print(\"三次式における'LSTAT'の住宅価格への決定係数は%.2f\"%(score_3d))\n",
    "print(\"四次式における'LSTAT'の住宅価格への決定係数は%.2f\"%(score_4d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 次数が大きくなるとどうなるか記述せよ\n",
    "以下の観点をすべて含めて記述しましょう。\n",
    "- 説明変数をxとして、次数を増やしていくとどのように数式が変化していくか記述せよ（1次式 ax + b）\n",
    "- 次数を増やすとどのようなメリットが考えられるか\n",
    "- 次数を増やすとどのようなデメリットが考えられるか"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 答え：\n",
    "説明変数をxとして、次数を増やしていくと数式は以下のように変化していく。  \n",
    "１次式：$ax + b$  \n",
    "２次式：$ax^2 + bx + c$  \n",
    "３次式：$ax^3 + bx^2 + cx + d$  \n",
    "４次式：$ax^4 + bx^3 + cx^2 + dx + e$  \n",
    "  \n",
    "次数を増やすと、モデルが複雑になって表現力を向上でき、その結果、誤差が減少するというメリットが考えられる。  \n",
    "次数を増やすと、モデルの表現力が高すぎるがゆえに、トレーニングデータのノイズまで含めて学習してしまう『過学習』が起きやすくなり、誤差が少なくてもモデルの汎用性は失われる、というデメリットが考えられる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重回帰\n",
    "今回は、LSTATのみを使用したが、他の特徴量も使用して学習させましょう。重回帰を使用して、0.71以上の決定係数出れば合格です。\n",
    "- すべての特徴量を使用せず、相関が強い特徴量のみを使用してみましょう。\n",
    "- 次数を変更してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.385832</td>\n",
       "      <td>0.360445</td>\n",
       "      <td>-0.483725</td>\n",
       "      <td>0.17526</td>\n",
       "      <td>-0.427321</td>\n",
       "      <td>0.69536</td>\n",
       "      <td>-0.376955</td>\n",
       "      <td>0.249929</td>\n",
       "      <td>-0.381626</td>\n",
       "      <td>-0.468536</td>\n",
       "      <td>-0.507787</td>\n",
       "      <td>0.333461</td>\n",
       "      <td>-0.737663</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CRIM        ZN     INDUS     CHAS       NOX       RM       AGE  \\\n",
       "0 -0.385832  0.360445 -0.483725  0.17526 -0.427321  0.69536 -0.376955   \n",
       "\n",
       "        DIS       RAD       TAX   PTRATIO         B     LSTAT    0  \n",
       "0  0.249929 -0.381626 -0.468536 -0.507787  0.333461 -0.737663  1.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 改めて、データフレームを作る\n",
    "X = pd.DataFrame(boston.data,columns=boston.feature_names)\n",
    "y = pd.DataFrame(boston.target)\n",
    "\n",
    "# Xとyを一旦合体して、相関をみる\n",
    "Xy = pd.concat([X, y], axis=1)\n",
    "Xy.corr().loc[[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- yに対して強い相関が見て取れる'RM'と'LSTAT'を用いて、１〜４次式で重回帰分析。\n",
    "- ２次式と３次式で決定係数0.65が出た。しかし、３次式以上の係数は過学習気味なので、取り敢えずは、２次式を採用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1次式における'RM'とLSTAT'の住宅価格への決定係数は0.54\n",
      "[[ 0.          5.10906846 -0.65494879]]\n",
      "2次式における'RM'とLSTAT'の住宅価格への決定係数は0.65\n",
      "[[  0.00000000e+00  -1.46867521e+01   5.12342260e-01   1.69773861e+00\n",
      "   -2.64734538e-01   1.08556128e-02]]\n",
      "3次式における'RM'とLSTAT'の住宅価格への決定係数は0.65\n",
      "[[  0.00000000e+00  -1.35603077e+02  -1.38317703e+01   1.86979322e+01\n",
      "    4.19021313e+00   1.30229313e-01  -7.69743126e-01  -3.45231795e-01\n",
      "   -1.66973729e-02  -5.14672730e-04]]\n",
      "4次式における'RM'とLSTAT'の住宅価格への決定係数は0.58\n",
      "[[  0.00000000e+00   7.16674537e+02  -7.06960369e+01  -2.11115569e+02\n",
      "    2.00539752e+01   2.46698151e+00   2.50194319e+01  -1.71426015e+00\n",
      "   -4.82578283e-01  -3.80916918e-02  -1.04097627e+00   3.61548220e-02\n",
      "    2.18842490e-02   3.92884465e-03   2.04857825e-04]]\n"
     ]
    }
   ],
   "source": [
    "# 採用した説明変数でデータを作る。ここで、すでにXは行列になっている\n",
    "MX = X.loc[:, ['RM','LSTAT']].as_matrix()\n",
    "\n",
    "# 学習データとテストデータに分割する\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(MX,y,test_size=0.2,random_state=0)\n",
    "\n",
    "# １〜４次式で重回帰分析\n",
    "lin_1d = LinearRegression()\n",
    "lin_2d = LinearRegression()\n",
    "lin_3d = LinearRegression()\n",
    "lin_4d = LinearRegression()\n",
    "\n",
    "degree_1 = PolynomialFeatures(degree=1)\n",
    "degree_2 = PolynomialFeatures(degree=2)\n",
    "degree_3 = PolynomialFeatures(degree=3)\n",
    "degree_4 = PolynomialFeatures(degree=4)\n",
    "\n",
    "x_train_1 = degree_1.fit_transform(X_train)\n",
    "x_train_2 = degree_2.fit_transform(X_train)\n",
    "x_train_3 = degree_3.fit_transform(X_train)\n",
    "x_train_4 = degree_4.fit_transform(X_train)\n",
    "\n",
    "lin_1d.fit(x_train_1, Y_train)\n",
    "lin_2d.fit(x_train_2, Y_train)\n",
    "lin_3d.fit(x_train_3, Y_train)\n",
    "lin_4d.fit(x_train_4, Y_train)\n",
    "\n",
    "x_test_1 = degree_1.fit_transform(X_test)\n",
    "x_test_2 = degree_2.fit_transform(X_test)\n",
    "x_test_3 = degree_3.fit_transform(X_test)\n",
    "x_test_4 = degree_4.fit_transform(X_test)\n",
    "\n",
    "score_1d = lin_1d.score(x_test_1, Y_test)\n",
    "score_2d = lin_2d.score(x_test_2, Y_test)\n",
    "score_3d = lin_3d.score(x_test_3, Y_test)\n",
    "score_4d = lin_4d.score(x_test_4, Y_test)\n",
    "\n",
    "print(\"1次式における'RM'とLSTAT'の住宅価格への決定係数は%.2f\"%(score_1d))\n",
    "print(lin_1d.coef_)\n",
    "print(\"2次式における'RM'とLSTAT'の住宅価格への決定係数は%.2f\"%(score_2d))\n",
    "print(lin_2d.coef_)\n",
    "print(\"3次式における'RM'とLSTAT'の住宅価格への決定係数は%.2f\"%(score_3d))\n",
    "print(lin_3d.coef_)\n",
    "print(\"4次式における'RM'とLSTAT'の住宅価格への決定係数は%.2f\"%(score_4d))\n",
    "print(lin_4d.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ２次式において愚直に説明変数を組み合わせてみた。\n",
    "- 'RM','LSTAT','DIS','ZN','AGE','RAD','TAX' で決定係数0.73超え。\n",
    "- もっとシステマティックにやる方法もあるのだろうけれども、今回はこれでいく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.736172773702\n"
     ]
    }
   ],
   "source": [
    "MX = X.loc[:, ['RM','LSTAT','DIS','ZN','AGE','RAD','TAX']].as_matrix()\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(MX,y,test_size=0.2,random_state=0)\n",
    "lin_ = LinearRegression()\n",
    "degree_ = PolynomialFeatures(degree=2)\n",
    "PX_train = degree_.fit_transform(X_train)\n",
    "lin_.fit(PX_train, Y_train)\n",
    "PX_test = degree_.fit_transform(X_test)\n",
    "score_ = lin_.score(PX_test, Y_test)\n",
    "print(score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- リッジ回帰では、正則化パラメータ0.00001、次数3で、決定係数0.76を超えた"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.766945774289\n"
     ]
    }
   ],
   "source": [
    "lin_ = Ridge(normalize=True,alpha=0.00001)\n",
    "degree_ = PolynomialFeatures(degree=3)\n",
    "PX_train = degree_.fit_transform(X_train)\n",
    "lin_.fit(PX_train, Y_train)\n",
    "PX_test = degree_.fit_transform(X_test)\n",
    "score_ = lin_.score(PX_test, Y_test)\n",
    "print(score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- クロスバリデーションとグリッドサーチにより、最も高い決定係数を出す次数と正則化パラメータを調べた\n",
    "- クロスバリデーション１０回のリッジ回帰で、「３次式」で、「正則化パラメータ0.000024」で、決定係数の平均は０．８４を超えた。しかし、これは本当に正しいのかどうか自信がない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  3.00000000e+00   2.40000000e-05   8.42928601e-01]\n"
     ]
    }
   ],
   "source": [
    "# クロスバリデーションの準備\n",
    "My = y.loc[:, [0]].as_matrix() # yも行列にする\n",
    "np.random.seed(0) # 乱数のシード設定\n",
    "indices = np.random.permutation(len(MX))\n",
    "rx = MX[indices] # データの順序をランダムに並び替え\n",
    "ry = My[indices]\n",
    "n_fold = 10 # クロスバリデーションの回数\n",
    "k_fold = cross_validation.KFold(n=len(rx),n_folds = n_fold)\n",
    "\n",
    "# グリッドサーチ結果のハイパーパラメータを格納する準備\n",
    "dim=np.arange(3,11) # 今のところ３次式以上で効果があることがわかっている\n",
    "lambda_=np.arange(1,30)/1e6 # 正則化パラメータは、効果があった0.00001周辺で考える\n",
    "\n",
    "# グリッドサーチで、ハイパーパラメータと決定係数を得る\n",
    "i = 0\n",
    "L=len(dim)*len(lambda_)\n",
    "score = np.zeros((L,3))\n",
    "tmp = []\n",
    "for d in dim:\n",
    "    degree_=PolynomialFeatures(degree=d)\n",
    "    for l in lambda_:\n",
    "        lin_ = Ridge(normalize=True,alpha=l)\n",
    "        for train, test in k_fold:\n",
    "            rx_train = degree_.fit_transform(rx[train])\n",
    "            lin_.fit(rx_train,ry[train])\n",
    "            \n",
    "            # 学習が終わったところで、テストデータで決定係数を得る\n",
    "            rx_test = degree_.fit_transform(rx[test])\n",
    "            tmp.append(lin_.score(rx_test, ry[test]))\n",
    "\n",
    "        score[i,0] = d\n",
    "        score[i,1] = l\n",
    "        \n",
    "        # 決定係数の平均を出してみる\n",
    "        score[i,2] = sum(tmp)/len(tmp)\n",
    "        i+=1\n",
    "        tmp = []\n",
    "\n",
    "# もっとも高い決定係数は、どの組み合わせで出て来るか\n",
    "print(score[np.argmax(score[:,-1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 少なくとも、学習データ80%、テストデータ20%では、決定係数0.84は出なかった"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.763643911396\n"
     ]
    }
   ],
   "source": [
    "lin_ = Ridge(normalize=True,alpha=0.000024)\n",
    "degree_ = PolynomialFeatures(degree=3)\n",
    "PX_train = degree_.fit_transform(X_train)\n",
    "lin_.fit(PX_train, Y_train)\n",
    "PX_test = degree_.fit_transform(X_test)\n",
    "score_ = lin_.score(PX_test, Y_test)\n",
    "print(score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重回帰について記述せよ\n",
    "以下の観点をすべて含めて記述しましょう。\n",
    "- 説明変数を増やすことでどのようなメリットがあるか\n",
    "- 説明変数を増やすことでどのようなデメリットがあるか"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 答え：\n",
    "説明変数を増やすことで、それらが適切な説明変数の組み合わせであった場合には、モデルの予測精度を向上できるメリットがある。  \n",
    "説明変数を増やすことで、  \n",
    "①それらが適切な説明変数の組み合わせでなかった場合には、モデルの予測精度が下がる。  \n",
    "②モデルの複雑さが増すため計算量が多くなる。  \n",
    "③グラフなどによる視覚的表現が難しくなるため、モデルの直感的な理解が難しくなる。  \n",
    "以上３つのようなデメリットがある。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
