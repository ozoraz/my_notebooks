{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 課題 ボストン住宅価格 線形回帰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 線形回帰とは何か\n",
    "以下の観点をすべて含めて記述しましょう。\n",
    "- 線形回帰とは何か。\n",
    "- 具体的に言うと？\n",
    "- 分類と何か違うのか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 答え：\n",
    "線形回帰とは、  \n",
    "Yが連続値の場合に、データにY = AX + Bというモデルを当てはめる回帰分析のひとつ。グラフに示す場合、途切れない線形を描く。 具体的には、特徴量XをモデルAX + Bに入力すればラベルYを推測できる、というもの。  \n",
    "分類は、Yが連続値ではない場合(離散)を扱う。グラフに示す場合、ある程度の刻み値を持った途切れ途切れの点や線形を描く。この***途切れないか、途切れるか***の点で、回帰と分類は扱う対象が異なる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 必要なライブラリをimportする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データを取得する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 取得データをDataFrameにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(boston.data,columns=boston.feature_names)\n",
    "y = pd.DataFrame(boston.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 説明変数を’LSTAT’のみにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X['LSTAT'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  単回帰と重回帰についての違いを記述せよ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 答え：\n",
    "上述した線形回帰モデルY = AX + Bにおいて、X が1次元ならば単回帰、X が2次元以上ならば重回帰と言う。機械学習においては、特徴量Xが１つだけなら単回帰、２つ以上なら重回帰である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テストデータに分割する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test=train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_1d = LinearRegression()\n",
    "lin_1d.fit(X_train[:,None], Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 決定係数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一次式における'LSTAT'の住宅価格への決定係数は0.43\n"
     ]
    }
   ],
   "source": [
    "score_1d = lin_1d.score(X_test[:,None], Y_test)\n",
    "print(\"一次式における'LSTAT'の住宅価格への決定係数は%.2f\"%(score_1d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 決定係数とは何か記述せよ\n",
    "以下の観点をすべて含めて記述しましょう。\n",
    "- 決定係数とは何か\n",
    "- もっとも説明変数が、目的変数を説明できる場合、決定係数は何になるか\n",
    "- どのように求めることができるか"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 答え：\n",
    "決定係数とは、  \n",
    "学習したモデルについて、説明変数が目的変数のどれくらいを説明できるかを表す値。未知のデータに対する予測精度を測る指標のひとつ。  \n",
    "***『もっとも説明変数が、目的変数を説明できる場合』***、つまり予測値と教師情報に誤差が生じない場合、決定係数は***1***になる。  \n",
    "(予測値−教師情報)の２乗和を(予測値−教師情報の平均)の２乗和で割ったものを、１から減算して、平方根を取って求めることができる(*この他にも、決定係数の定義は複数ある)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 決定係数をいかなる場合も信じても良いか記述せよ(決定係数が高ければ、汎用性があるモデルと言えるか)\n",
    "- 決定係数が正しく評価できない例を答えよ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 答え：\n",
    "決定係数が高ければ、汎用性があるモデルとは、必ずしも言えない。何故ならば、決定係数はあくまで***予め用意できた教師情報に対するモデルの当てはまり***しか示せないからである。つまり、用意できた教師情報以外へのモデルの当てはまりは保証できないので、教師情報以外の巨大な母集団に対してはモデルが当てはまらない可能性がある。  \n",
    "例えば、選挙の出口調査で１００人から投票先を聞いた場合を考える。このうち８０人のデータをトレーニングデータに学習してモデルを作り、２０人のデータをテストデータとして決定係数をとる。ここで、決定係数が高いからといってこのモデルが全体の選挙結果に対して汎用性がある保証はない。何故ならば、モデルの当てはまりが良いのは、あくまで***出口調査した１００人についてのみ***であるからだ。全体の選挙結果について汎用性を求めるなら、この方法では***日本の有権者人口約１億人全てについて***出口調査する必要がある"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2,3,4次式の回帰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "二次式における'LSTAT'の住宅価格への決定係数は0.52\n",
      "三次式における'LSTAT'の住宅価格への決定係数は0.54\n",
      "四次式における'LSTAT'の住宅価格への決定係数は0.57\n"
     ]
    }
   ],
   "source": [
    "lin_2d = LinearRegression()\n",
    "lin_3d = LinearRegression()\n",
    "lin_4d = LinearRegression()\n",
    "\n",
    "degree_2 = PolynomialFeatures(degree=2)\n",
    "degree_3 = PolynomialFeatures(degree=3)\n",
    "degree_4 = PolynomialFeatures(degree=4)\n",
    "\n",
    "x_train_2 = degree_2.fit_transform(X_train[:,None])\n",
    "x_train_3 = degree_3.fit_transform(X_train[:,None])\n",
    "x_train_4 = degree_4.fit_transform(X_train[:,None])\n",
    "\n",
    "lin_2d.fit(x_train_2, Y_train)\n",
    "lin_3d.fit(x_train_3, Y_train)\n",
    "lin_4d.fit(x_train_4, Y_train)\n",
    "\n",
    "x_test_2 = degree_2.fit_transform(X_test[:,None])\n",
    "x_test_3 = degree_3.fit_transform(X_test[:,None])\n",
    "x_test_4 = degree_4.fit_transform(X_test[:,None])\n",
    "\n",
    "score_2d = lin_2d.score(x_test_2, Y_test)\n",
    "score_3d = lin_3d.score(x_test_3, Y_test)\n",
    "score_4d = lin_4d.score(x_test_4, Y_test)\n",
    "\n",
    "print(\"二次式における'LSTAT'の住宅価格への決定係数は%.2f\"%(score_2d))\n",
    "print(\"三次式における'LSTAT'の住宅価格への決定係数は%.2f\"%(score_3d))\n",
    "print(\"四次式における'LSTAT'の住宅価格への決定係数は%.2f\"%(score_4d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 次数が大きくなるとどうなるか記述せよ\n",
    "以下の観点をすべて含めて記述しましょう。\n",
    "- 説明変数をxとして、次数を増やしていくとどのように数式が変化していくか記述せよ（1次式 ax + b）\n",
    "- 次数を増やすとどのようなメリットが考えられるか\n",
    "- 次数を増やすとどのようなデメリットが考えられるか"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 答え：\n",
    "説明変数をxとして、次数を増やしていくと数式は以下のように変化していく。  \n",
    "１次式：$ax + b$  \n",
    "２次式：$ax^2 + bx + c$  \n",
    "３次式：$ax^3 + bx^2 + cx + d$  \n",
    "４次式：$ax^4 + bx^3 + cx^2 + dx + e$  \n",
    "  \n",
    "次数を増やすと、モデルが複雑になって表現力を向上でき、その結果、誤差が減少するというメリットが考えられる。  \n",
    "次数を増やすと、モデルの表現力が高すぎるがゆえに、トレーニングデータのノイズまで含めて学習してしまう『過学習』が起きやすくなり、誤差が少なくてもモデルの汎用性は失われる、というデメリットが考えられる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重回帰\n",
    "今回は、LSTATのみを使用したが、他の特徴量も使用して学習させましょう。重回帰を使用して、0.71以上の決定係数出れば合格です。\n",
    "- すべての特徴量を使用せず、相関が強い特徴量のみを使用してみましょう。\n",
    "- 次数を変更してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.385832</td>\n",
       "      <td>0.360445</td>\n",
       "      <td>-0.483725</td>\n",
       "      <td>0.17526</td>\n",
       "      <td>-0.427321</td>\n",
       "      <td>0.69536</td>\n",
       "      <td>-0.376955</td>\n",
       "      <td>0.249929</td>\n",
       "      <td>-0.381626</td>\n",
       "      <td>-0.468536</td>\n",
       "      <td>-0.507787</td>\n",
       "      <td>0.333461</td>\n",
       "      <td>-0.737663</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CRIM        ZN     INDUS     CHAS       NOX       RM       AGE  \\\n",
       "0 -0.385832  0.360445 -0.483725  0.17526 -0.427321  0.69536 -0.376955   \n",
       "\n",
       "        DIS       RAD       TAX   PTRATIO         B     LSTAT    0  \n",
       "0  0.249929 -0.381626 -0.468536 -0.507787  0.333461 -0.737663  1.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 改めて、データフレームを作る\n",
    "X = pd.DataFrame(boston.data,columns=boston.feature_names)\n",
    "y = pd.DataFrame(boston.target)\n",
    "\n",
    "# Xとyを一旦合体して、相関をみる\n",
    "Xy = pd.concat([X, y], axis=1)\n",
    "Xy.corr().loc[[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 相関をみたけれども、どの特徴量を選択したものかピンと来ない。itertools.combinationsで、特徴量の組み合わせを総当たりで調べる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8190"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 特徴量のカラム名を取得\n",
    "feat_names = list(boston.feature_names)\n",
    "\n",
    "# カラム数毎に、全ての組み合わせを取得\n",
    "comb = []\n",
    "for i in range(1, len(feat_names)):\n",
    "    comb.append(itertools.combinations(feat_names, i))\n",
    "\n",
    "# 全ての組み合わせを文字列リストのリストに変換\n",
    "comb_str = []\n",
    "for i in range(len(comb)):\n",
    "    for j in comb[i]:\n",
    "        comb_str.append(list(map(str, j)))\n",
    "        \n",
    "len(comb_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 特徴量の全ての組み合わせについて１次式の重回帰分析。\n",
    "- ZN, CHAS, NOX, RM, AGE, DIS, B, LSTATの組み合わせが、決定係数0.609504で最高だった。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comb</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6822</th>\n",
       "      <td>[ZN, CHAS, NOX, RM, AGE, DIS, B, LSTAT]</td>\n",
       "      <td>0.609504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5295</th>\n",
       "      <td>[ZN, CHAS, NOX, RM, DIS, B, LSTAT]</td>\n",
       "      <td>0.608822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5350</th>\n",
       "      <td>[ZN, CHAS, RM, AGE, DIS, B, LSTAT]</td>\n",
       "      <td>0.605801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         comb     score\n",
       "6822  [ZN, CHAS, NOX, RM, AGE, DIS, B, LSTAT]  0.609504\n",
       "5295       [ZN, CHAS, NOX, RM, DIS, B, LSTAT]  0.608822\n",
       "5350       [ZN, CHAS, RM, AGE, DIS, B, LSTAT]  0.605801"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# １次式の重回帰分析。組み合わせと決定係数をcomb_scoreに格納\n",
    "comb_score = []\n",
    "for l in comb_str:\n",
    "    MX = X.loc[:, l].as_matrix()\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(MX,y,test_size=0.2,random_state=0)\n",
    "    lin_1d = LinearRegression()\n",
    "    lin_1d.fit(X_train, Y_train)\n",
    "    score_1d = lin_1d.score(X_test, Y_test)\n",
    "    comb_score.append([l, score_1d])\n",
    "\n",
    "# comb_scoreをDataFrameにして、決定係数が最大になる組み合わせを出す\n",
    "pd.DataFrame(comb_score, columns=['comb', 'score'])\\\n",
    "    .sort_values(by=['score'], ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 特徴量の全ての組み合わせについて２次式の重回帰分析。\n",
    "- CRIM, ZN, INDUS, CHAS, NOX, RM, DIS, RAD, TAX, LSTATの組み合わせが、決定係数0.795047で最高だった。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comb</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7835</th>\n",
       "      <td>[CRIM, ZN, INDUS, CHAS, NOX, RM, DIS, RAD, TAX, LSTAT]</td>\n",
       "      <td>0.795047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7935</th>\n",
       "      <td>[CRIM, ZN, CHAS, NOX, RM, AGE, DIS, RAD, TAX, LSTAT]</td>\n",
       "      <td>0.791046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6505</th>\n",
       "      <td>[CRIM, CHAS, NOX, RM, DIS, RAD, TAX, LSTAT]</td>\n",
       "      <td>0.785194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        comb     score\n",
       "7835  [CRIM, ZN, INDUS, CHAS, NOX, RM, DIS, RAD, TAX, LSTAT]  0.795047\n",
       "7935    [CRIM, ZN, CHAS, NOX, RM, AGE, DIS, RAD, TAX, LSTAT]  0.791046\n",
       "6505             [CRIM, CHAS, NOX, RM, DIS, RAD, TAX, LSTAT]  0.785194"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ２次式の重回帰分析。組み合わせと決定係数をcomb_scoreに格納\n",
    "comb_score = []\n",
    "for l in comb_str:\n",
    "    MX = X.loc[:, l].as_matrix()\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(MX,y,test_size=0.2,random_state=0)\n",
    "    lin_ = LinearRegression()\n",
    "    degree_ = PolynomialFeatures(degree=2)\n",
    "    PX_train = degree_.fit_transform(X_train)\n",
    "    lin_.fit(PX_train, Y_train)\n",
    "    PX_test = degree_.fit_transform(X_test)\n",
    "    score_ = lin_.score(PX_test, Y_test)\n",
    "    comb_score.append([l, score_])\n",
    "\n",
    "# 文字が省略されてしまうので設定\n",
    "pd.set_option(\"display.max_colwidth\", 100)\n",
    "\n",
    "# comb_scoreをDataFrameにして、決定係数が最大になる組み合わせを出す\n",
    "pd.DataFrame(comb_score, columns=['comb', 'score'])\\\n",
    "    .sort_values(by=['score'], ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 特徴量の全ての組み合わせについて、クロスバリデーション１０回による、リッジ回帰分析。\n",
    "- 最も高い決定係数を出す組み合わせと正則化パラメータを調べた。本当は次数を１０次式まで調べたかったが、１０時間以上かけても処理が終わらないため、ここでは３次式に絞ることとした。\n",
    "- ３次式で２時間以上の処理になったが、結果は奮わなかった。機械学習の計算量と処理の重さを思い知った。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comb</th>\n",
       "      <th>param</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[ZN, INDUS, CHAS, NOX, RM, AGE, DIS, RAD, TAX, PTRATIO, B, LSTAT]</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.580640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[ZN, INDUS, CHAS, NOX, RM, AGE, DIS, RAD, TAX, PTRATIO, B, LSTAT]</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.572099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[ZN, INDUS, CHAS, NOX, RM, AGE, DIS, RAD, TAX, PTRATIO, B, LSTAT]</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.562950</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 comb  \\\n",
       "18  [ZN, INDUS, CHAS, NOX, RM, AGE, DIS, RAD, TAX, PTRATIO, B, LSTAT]   \n",
       "17  [ZN, INDUS, CHAS, NOX, RM, AGE, DIS, RAD, TAX, PTRATIO, B, LSTAT]   \n",
       "16  [ZN, INDUS, CHAS, NOX, RM, AGE, DIS, RAD, TAX, PTRATIO, B, LSTAT]   \n",
       "\n",
       "       param     score  \n",
       "18  0.000019  0.580640  \n",
       "17  0.000018  0.572099  \n",
       "16  0.000017  0.562950  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for comb in comb_str:\n",
    "    MX = X.loc[:, comb].as_matrix()\n",
    "\n",
    "    # クロスバリデーションの準備\n",
    "    My = y.loc[:, [0]].as_matrix() # yも行列にする\n",
    "    np.random.seed(0) # 乱数のシード設定\n",
    "    indices = np.random.permutation(len(My))\n",
    "    rx = MX[indices] \n",
    "    ry = My[indices]\n",
    "    n_fold = 10 \n",
    "    k_fold = cross_validation.KFold(n=len(rx),n_folds = n_fold)\n",
    "\n",
    "    lambda_=np.arange(1,20)/1e6 # 正則化パラメータは0.00001周辺で考える\n",
    "\n",
    "    score = []\n",
    "    tmp = []\n",
    "    degree_=PolynomialFeatures(degree=3)\n",
    "    for l in lambda_:\n",
    "        lin_ = Ridge(normalize=True,alpha=l)\n",
    "        for train, test in k_fold:\n",
    "            rx_train = degree_.fit_transform(rx[train])\n",
    "            lin_.fit(rx_train,ry[train])\n",
    "\n",
    "            # 学習が終わったところで、テストデータで決定係数を得る\n",
    "            rx_test = degree_.fit_transform(rx[test])\n",
    "            tmp.append(lin_.score(rx_test, ry[test]))\n",
    "\n",
    "        # 決定係数の平均をとる\n",
    "        r_mean = sum(tmp)/len(tmp)\n",
    "        score.append([comb, l, r_mean])\n",
    "\n",
    "        tmp = []\n",
    "\n",
    "# scoreをDataFrameにして、決定係数が最大になる組み合わせを出す\n",
    "pd.DataFrame(score, columns=['comb', 'param', 'score'])\\\n",
    "    .sort_values(by=['score'], ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重回帰について記述せよ\n",
    "以下の観点をすべて含めて記述しましょう。\n",
    "- 説明変数を増やすことでどのようなメリットがあるか\n",
    "- 説明変数を増やすことでどのようなデメリットがあるか"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 答え：\n",
    "説明変数を増やすことで、それらが適切な説明変数の組み合わせであった場合には、モデルの予測精度を向上できるメリットがある。  \n",
    "説明変数を増やすことで、  \n",
    "①それらが適切な説明変数の組み合わせでなかった場合には、モデルの予測精度が下がる。  \n",
    "②モデルの複雑さが増すため計算量が多くなる。  \n",
    "③グラフなどによる視覚的表現が難しくなるため、モデルの直感的な理解が難しくなる。  \n",
    "以上３つのようなデメリットがある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(comb_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
